GRU Model
lookback: 40
hidden_dim: 64
num_layers: 1
epochs: 100
learning_rate: 0.001
batch_size: 32
criterion: MSELoss
optimizer: Adam
scheduler: ReduceLROnPlateau
dropout: 0.2
patience: 10

train_mse: 0.86
test_mse: 18.7
train_rmse: 0.93
test_rmse: 4.32
train_mape: 2.94
test_mape: 4.57
--------------------------------
GRU Model
lookback: 60
hidden_dim: 64
num_layers: 1
epochs: 100
learning_rate: 0.001
batch_size: 32
criterion: MSELoss
optimizer: Adam
scheduler: ReduceLROnPlateau
dropout: 0.2
patience: 10

train_mse: 0.03
test_mse: 0.86
train_rmse: 0.16
test_rmse: 0.93
train_mape: 0.38
test_mape: 0.95
--------------------------------
GRU Model
lookback: 40
hidden_dim: 96
num_layers: 1
epochs: 100
learning_rate: 0.001
batch_size: 16
criterion: MSELoss
optimizer: Adam
scheduler: ReduceLROnPlateau
dropout: 0.5
patience: 10

train_mse: 0.11
test_mse: 1.17
train_rmse: 0.33
test_rmse: 1.08
train_mape: 1.08
test_mape: 1.54
--------------------------------
GRU Model
lookback: 7
hidden_dim: 64
num_layers: 1
epochs: 100
learning_rate: 0.1
batch_size: 16
criterion: MSELoss
optimizer: Adam
scheduler: ReduceLROnPlateau
dropout: 0.5
patience: 10

train_mse: 0.3
test_mse: 3.93
train_rmse: 0.55
test_rmse: 1.98
train_mape: 1.41
test_mape: 2.09
--------------------------------
GRU Model
lookback: 7
hidden_dim: 64
num_layers: 1
epochs: 100
learning_rate: 0.001
batch_size: 16
criterion: MSELoss
optimizer: Adam
scheduler: ReduceLROnPlateau
dropout: 0.5
patience: 10

train_mse: 0.18
test_mse: 2.02
train_rmse: 0.42
test_rmse: 1.42
train_mape: 1.36
test_mape: 1.64
--------------------------------
GRU Model
lookback: 7
hidden_dim: 64
num_layers: 1
epochs: 20
learning_rate: 0.001
batch_size: 16
criterion: MSELoss
optimizer: Adam
scheduler: ReduceLROnPlateau
dropout: 0.5
patience: 10

train_mse: 1.89
test_mse: 12.78
train_rmse: 1.38
test_rmse: 3.57
train_mape: 4.7
test_mape: 4.34
--------------------------------
GRU Model
lookback: 7
hidden_dim: 64
num_layers: 1
epochs: 50
learning_rate: 0.001
batch_size: 16
criterion: MSELoss
optimizer: Adam
scheduler: ReduceLROnPlateau
dropout: 0.5
patience: 10

train_mse: 1.72
test_mse: 0.49
train_rmse: 1.31
test_rmse: 0.7
train_mape: 4.4
test_mape: 0.87
--------------------------------
GRU Model
lookback: 20
hidden_dim: 64
num_layers: 1
epochs: 50
learning_rate: 0.001
batch_size: 16
criterion: MSELoss
optimizer: Adam
scheduler: ReduceLROnPlateau
dropout: 0.5
patience: 10

train_mse: 0.83
test_mse: 0.84
train_rmse: 0.91
test_rmse: 0.91
train_mape: 3.03
test_mape: 1.49
--------------------------------
GRU Model
lookback: 20
hidden_dim: 64
num_layers: 1
epochs: 50
learning_rate: 0.001
batch_size: 16
criterion: MSELoss
optimizer: Adam
scheduler: None
dropout: 0.2
patience: 10

train_mse: 2.63
test_mse: 0.55
train_rmse: 1.62
test_rmse: 0.74
train_mape: 5.59
test_mape: 0.97
--------------------------------
GRU Model
lookback: 20
hidden_dim: 64
num_layers: 1
epochs: 100
learning_rate: 0.001
batch_size: 16
criterion: MSELoss
optimizer: Adam
scheduler: None
dropout: 0.2
patience: 10

train_mse: 0.95
test_mse: 5.15
train_rmse: 0.97
test_rmse: 2.27
train_mape: 2.75
test_mape: 2.74
--------------------------------
GRU Model
lookback: 20
hidden_dim: 64
num_layers: 1
epochs: 100
learning_rate: 0.001
batch_size: 16
criterion: MSELoss
optimizer: Adam
scheduler: ReduceLROnPlateau
dropout: 0.2
patience: 10

train_mse: 0.10
test_mse: 0.28
train_rmse: 0.32
test_rmse: 0.53
train_mape: 1.11
test_mape: 0.66
--------------------------------
